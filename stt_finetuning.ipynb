{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech-to-Text Model Fine-Tuning Pipeline\n",
    "\n",
    "A configurable notebook for fine-tuning various STT models (Whisper, Parakeet, and more) on custom datasets.\n",
    "\n",
    "## Features\n",
    "- **Multi-model support**: Whisper, Parakeet, and extensible architecture for future models\n",
    "- **Unified dataset handling**: Common dataset format for all models\n",
    "- **Flexible data splitting**: Train/validation/test splits or separate datasets\n",
    "- **Parallel training**: Optional concurrent training of multiple models\n",
    "- **Colab compatible**: Designed to run on Google Colab with GPU support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Google Colab Setup (Run this first if on Colab)\nimport sys\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    print(\"Running on Google Colab\")\n    print(\"Mounting Google Drive for data persistence...\")\n    from google.colab import drive\n    drive.mount('/content/drive')\nelse:\n    print(\"Running locally\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n# IMPORTANT: Run this cell, then RESTART RUNTIME before running subsequent cells\n\nINSTALL_PACKAGES = True  # Set to False if packages are already installed\n\nif INSTALL_PACKAGES:\n    import sys\n    IN_COLAB = 'google.colab' in sys.modules\n    \n    if IN_COLAB:\n        # Fix NumPy compatibility issue on Colab\n        print(\"Step 1/4: Upgrading NumPy to fix compatibility issues...\")\n        !pip uninstall -y numpy\n        !pip install numpy>=1.26.0\n        \n        print(\"\\nStep 2/4: Installing audio decoding dependencies...\")\n        !pip install -q torchcodec soundfile librosa\n        \n        print(\"\\nStep 3/4: Installing core packages...\")\n        !pip install -q --upgrade transformers datasets accelerate evaluate jiwer\n        !pip install -q torch torchaudio --upgrade\n        !pip install -q huggingface_hub\n        \n        print(\"\\nStep 4/4: Installing NeMo for Parakeet (optional, may take a while)...\")\n        # Uncomment the line below if you need Parakeet support\n        # !pip install -q nemo_toolkit[asr]\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"IMPORTANT: Please restart the runtime now!\")\n        print(\"Go to: Runtime -> Restart runtime\")\n        print(\"Then run cells starting from the imports cell (skip this cell)\")\n        print(\"=\"*60)\n        \n        # Auto-restart option for Colab\n        # Uncomment the lines below to auto-restart\n        # import os\n        # os.kill(os.getpid(), 9)\n    else:\n        # Local installation\n        !pip install -q transformers datasets accelerate evaluate jiwer\n        !pip install -q torch torchaudio torchcodec\n        !pip install -q soundfile librosa\n        !pip install -q huggingface_hub\n        # !pip install -q nemo_toolkit[asr]  # Uncomment for Parakeet support\n        print(\"Packages installed successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core imports\n# NOTE: If you see NumPy errors, restart the runtime first (Runtime -> Restart runtime)\n\nimport os\nimport json\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Union, Any, Callable\nfrom dataclasses import dataclass, field, asdict\nfrom abc import ABC, abstractmethod\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\nimport multiprocessing as mp\n\nimport numpy as np\nprint(f\"NumPy version: {np.__version__}\")\n\nimport torch\nimport torchaudio\nfrom datasets import Dataset, DatasetDict, Audio, load_dataset, concatenate_datasets\nfrom transformers import (\n    WhisperProcessor,\n    WhisperForConditionalGeneration,\n    Seq2SeqTrainer,\n    Seq2SeqTrainingArguments,\n    TrainerCallback,\n)\nimport evaluate\n\n# Detect Colab\nimport sys\nIN_COLAB = 'google.colab' in sys.modules\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\nprint(\"\\nAll imports successful!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "All configurable parameters are centralized here for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "@dataclass\nclass DatasetConfig:\n    \"\"\"Configuration for dataset handling.\"\"\"\n    # Primary dataset sources (can be HuggingFace dataset names or local paths)\n    train_datasets: List[str] = field(default_factory=lambda: [\"mozilla-foundation/common_voice_11_0\"])\n    \n    # Optional separate validation/test datasets (if None, will split from train)\n    validation_dataset: Optional[str] = None\n    test_dataset: Optional[str] = None\n    \n    # Dataset split ratios (used if validation/test datasets not provided)\n    train_split: float = 0.8\n    validation_split: float = 0.1\n    test_split: float = 0.1\n    \n    # Column names in dataset\n    audio_column: str = \"audio\"\n    text_column: str = \"sentence\"\n    \n    # Audio settings\n    sampling_rate: int = 16000\n    max_audio_length_seconds: float = 30.0\n    min_audio_length_seconds: float = 0.5\n    \n    # HuggingFace dataset settings\n    # For datasets like Common Voice that require a config name (e.g., language code)\n    dataset_config_name: Optional[str] = \"en\"  # e.g., \"en\" for Common Voice\n    dataset_split: str = \"train\"  # Which split to load\n    \n    # Data limits (for quick testing)\n    max_train_samples: Optional[int] = None\n    max_val_samples: Optional[int] = None\n    max_test_samples: Optional[int] = None\n    \n    # Trust remote code (for some HuggingFace datasets)\n    trust_remote_code: bool = True"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for a single model.\"\"\"\n",
    "    model_type: str  # \"whisper\", \"parakeet\", etc.\n",
    "    model_name: str  # e.g., \"openai/whisper-small\", \"nvidia/parakeet-ctc-1.1b\"\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    learning_rate: float = 1e-5\n",
    "    batch_size: int = 8\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    num_epochs: int = 3\n",
    "    warmup_steps: int = 500\n",
    "    weight_decay: float = 0.01\n",
    "    \n",
    "    # Model-specific settings\n",
    "    freeze_encoder: bool = False\n",
    "    freeze_encoder_layers: int = 0  # Number of encoder layers to freeze (0 = none)\n",
    "    \n",
    "    # Output settings\n",
    "    output_dir: str = \"./outputs\"\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 500\n",
    "    logging_steps: int = 100\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False\n",
    "    \n",
    "    # Additional kwargs for model-specific configurations\n",
    "    extra_kwargs: Dict[str, Any] = field(default_factory=dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Master configuration for the training pipeline.\"\"\"\n",
    "    # Models to train\n",
    "    models: List[ModelConfig] = field(default_factory=list)\n",
    "    \n",
    "    # Dataset configuration\n",
    "    dataset: DatasetConfig = field(default_factory=DatasetConfig)\n",
    "    \n",
    "    # Parallel training settings\n",
    "    enable_parallel: bool = False\n",
    "    max_parallel_models: int = 2  # Max models to train in parallel\n",
    "    \n",
    "    # General settings\n",
    "    seed: int = 42\n",
    "    output_base_dir: str = \"./outputs\"\n",
    "    \n",
    "    # Evaluation settings\n",
    "    compute_wer: bool = True\n",
    "    compute_cer: bool = True\n",
    "    \n",
    "    # Checkpointing\n",
    "    resume_from_checkpoint: Optional[str] = None\n",
    "    save_total_limit: int = 3\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        Path(self.output_base_dir).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# MAIN CONFIGURATION - MODIFY THIS SECTION\n# ============================================================================\n\n# Define models to fine-tune\nMODELS_TO_TRAIN = [\n    ModelConfig(\n        model_type=\"whisper\",\n        model_name=\"openai/whisper-small\",  # Use smaller model for testing\n        learning_rate=1e-5,\n        batch_size=8,\n        num_epochs=3,\n        output_dir=\"./outputs/whisper-small-finetuned\",\n        freeze_encoder=False,\n    ),\n    # Uncomment to add more models:\n    # ModelConfig(\n    #     model_type=\"whisper\",\n    #     model_name=\"openai/whisper-medium\",\n    #     learning_rate=5e-6,\n    #     batch_size=4,\n    #     num_epochs=3,\n    #     output_dir=\"./outputs/whisper-medium-finetuned\",\n    # ),\n    # ModelConfig(\n    #     model_type=\"parakeet\",\n    #     model_name=\"nvidia/parakeet-ctc-1.1b\",\n    #     learning_rate=1e-5,\n    #     batch_size=8,\n    #     num_epochs=3,\n    #     output_dir=\"./outputs/parakeet-finetuned\",\n    # ),\n]\n\n# Dataset configuration\nDATASET_CONFIG = DatasetConfig(\n    # Add your dataset sources here\n    train_datasets=[\n        \"mozilla-foundation/common_voice_11_0\",\n        # Add more datasets to combine them:\n        # \"librispeech_asr\",\n        # \"/path/to/local/dataset\",\n    ],\n    \n    # Optional: Provide separate validation/test datasets\n    # If None, will split from train_datasets\n    validation_dataset=None,\n    test_dataset=None,\n    \n    # Split ratios (used if validation/test datasets not provided)\n    train_split=0.8,\n    validation_split=0.1,\n    test_split=0.1,\n    \n    # Column names (adjust based on your dataset)\n    audio_column=\"audio\",\n    text_column=\"sentence\",\n    \n    # HuggingFace dataset settings\n    # For Common Voice: use language code like \"en\", \"es\", \"fr\", etc.\n    # For other datasets: set to None if no config name needed\n    dataset_config_name=\"en\",\n    dataset_split=\"train\",\n    \n    # Limit samples for testing (set to None for full dataset)\n    max_train_samples=1000,  # Set to None for full dataset\n    max_val_samples=100,\n    max_test_samples=100,\n    \n    # Trust remote code for some HuggingFace datasets\n    trust_remote_code=True,\n)\n\n# Master configuration\nCONFIG = TrainingConfig(\n    models=MODELS_TO_TRAIN,\n    dataset=DATASET_CONFIG,\n    \n    # Parallel training\n    enable_parallel=False,  # Set to True to train models in parallel\n    max_parallel_models=2,\n    \n    # General settings\n    seed=42,\n    output_base_dir=\"./outputs\",\n    \n    # Evaluation\n    compute_wer=True,\n    compute_cer=True,\n)\n\n# Colab-specific paths\nif IN_COLAB:\n    CONFIG.output_base_dir = \"/content/drive/MyDrive/stt-finetuning/outputs\"\n    for model in CONFIG.models:\n        model.output_dir = f\"/content/drive/MyDrive/stt-finetuning/{model.model_name.split('/')[-1]}-finetuned\"\n\nprint(\"Configuration loaded successfully!\")\nprint(f\"Models to train: {[m.model_name for m in CONFIG.models]}\")\nprint(f\"Dataset: {CONFIG.dataset.train_datasets}\")\nprint(f\"Dataset config: {CONFIG.dataset.dataset_config_name}\")\nprint(f\"Parallel training: {CONFIG.enable_parallel}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Handler\n",
    "\n",
    "Unified dataset loading, preprocessing, and splitting for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class DatasetHandler:\n    \"\"\"Handles dataset loading, preprocessing, and splitting.\"\"\"\n    \n    def __init__(self, config: DatasetConfig):\n        self.config = config\n        self._datasets: Optional[DatasetDict] = None\n    \n    def load_single_dataset(self, source: str) -> Dataset:\n        \"\"\"Load a single dataset from HuggingFace or local path.\"\"\"\n        logger.info(f\"Loading dataset from: {source}\")\n        \n        if os.path.exists(source):\n            # Local dataset\n            if source.endswith('.json') or source.endswith('.jsonl'):\n                dataset = load_dataset('json', data_files=source, split='train')\n            elif source.endswith('.csv'):\n                dataset = load_dataset('csv', data_files=source, split='train')\n            elif os.path.isdir(source):\n                dataset = load_dataset('audiofolder', data_dir=source, split='train')\n            else:\n                raise ValueError(f\"Unsupported local dataset format: {source}\")\n        else:\n            # HuggingFace dataset\n            logger.info(f\"Loading HuggingFace dataset: {source}, config: {self.config.dataset_config_name}, split: {self.config.dataset_split}\")\n            \n            try:\n                # Try loading with config name (for datasets like Common Voice)\n                if self.config.dataset_config_name:\n                    dataset = load_dataset(\n                        source,\n                        self.config.dataset_config_name,\n                        split=self.config.dataset_split,\n                        trust_remote_code=self.config.trust_remote_code,\n                    )\n                else:\n                    dataset = load_dataset(\n                        source,\n                        split=self.config.dataset_split,\n                        trust_remote_code=self.config.trust_remote_code,\n                    )\n            except Exception as e:\n                logger.warning(f\"Failed to load with config name, trying without: {e}\")\n                # Fallback: try loading without config name\n                dataset = load_dataset(\n                    source,\n                    split=self.config.dataset_split,\n                    trust_remote_code=self.config.trust_remote_code,\n                )\n            \n            # Handle DatasetDict if returned\n            if isinstance(dataset, DatasetDict):\n                available_splits = list(dataset.keys())\n                logger.info(f\"Available splits: {available_splits}\")\n                dataset = dataset[available_splits[0]]\n        \n        logger.info(f\"Loaded {len(dataset)} samples\")\n        return dataset\n    \n    def combine_datasets(self, datasets: List[Dataset]) -> Dataset:\n        \"\"\"Combine multiple datasets into one.\"\"\"\n        if len(datasets) == 1:\n            return datasets[0]\n        \n        # Standardize column names across datasets\n        standardized = []\n        for ds in datasets:\n            # Rename columns if needed\n            if self.config.audio_column not in ds.column_names:\n                # Try common audio column names\n                for col in ['audio', 'file', 'path', 'audio_path']:\n                    if col in ds.column_names:\n                        ds = ds.rename_column(col, self.config.audio_column)\n                        break\n            \n            if self.config.text_column not in ds.column_names:\n                # Try common text column names\n                for col in ['sentence', 'text', 'transcription', 'transcript']:\n                    if col in ds.column_names:\n                        ds = ds.rename_column(col, self.config.text_column)\n                        break\n            \n            # Keep only necessary columns\n            cols_to_keep = [self.config.audio_column, self.config.text_column]\n            cols_to_keep = [c for c in cols_to_keep if c in ds.column_names]\n            ds = ds.select_columns(cols_to_keep)\n            standardized.append(ds)\n        \n        return concatenate_datasets(standardized)\n    \n    def filter_by_audio_length(self, dataset: Dataset) -> Dataset:\n        \"\"\"Filter samples by audio length.\"\"\"\n        def is_valid_length(example):\n            audio = example[self.config.audio_column]\n            if isinstance(audio, dict) and 'array' in audio:\n                duration = len(audio['array']) / audio.get('sampling_rate', self.config.sampling_rate)\n            else:\n                return True  # Can't determine length, keep sample\n            \n            return self.config.min_audio_length_seconds <= duration <= self.config.max_audio_length_seconds\n        \n        return dataset.filter(is_valid_length)\n    \n    def prepare_datasets(self) -> DatasetDict:\n        \"\"\"Load and prepare all datasets with train/val/test splits.\"\"\"\n        if self._datasets is not None:\n            return self._datasets\n        \n        # Load and combine training datasets\n        train_datasets = [self.load_single_dataset(src) for src in self.config.train_datasets]\n        combined_train = self.combine_datasets(train_datasets)\n        \n        logger.info(f\"Combined dataset columns: {combined_train.column_names}\")\n        \n        # Cast audio column\n        if self.config.audio_column in combined_train.column_names:\n            combined_train = combined_train.cast_column(\n                self.config.audio_column, \n                Audio(sampling_rate=self.config.sampling_rate)\n            )\n        \n        # Filter by audio length\n        logger.info(\"Filtering by audio length...\")\n        combined_train = self.filter_by_audio_length(combined_train)\n        logger.info(f\"After filtering: {len(combined_train)} samples\")\n        \n        # Handle validation dataset\n        if self.config.validation_dataset:\n            validation = self.load_single_dataset(self.config.validation_dataset)\n            validation = validation.cast_column(\n                self.config.audio_column,\n                Audio(sampling_rate=self.config.sampling_rate)\n            )\n        else:\n            validation = None\n        \n        # Handle test dataset\n        if self.config.test_dataset:\n            test = self.load_single_dataset(self.config.test_dataset)\n            test = test.cast_column(\n                self.config.audio_column,\n                Audio(sampling_rate=self.config.sampling_rate)\n            )\n        else:\n            test = None\n        \n        # Split if validation/test not provided\n        if validation is None or test is None:\n            logger.info(\"Splitting dataset into train/val/test...\")\n            # First split: train + (val + test)\n            split1 = combined_train.train_test_split(\n                test_size=(self.config.validation_split + self.config.test_split),\n                seed=42\n            )\n            train = split1['train']\n            \n            # Second split: val + test\n            val_test_ratio = self.config.test_split / (self.config.validation_split + self.config.test_split)\n            split2 = split1['test'].train_test_split(test_size=val_test_ratio, seed=42)\n            \n            if validation is None:\n                validation = split2['train']\n            if test is None:\n                test = split2['test']\n        else:\n            train = combined_train\n        \n        # Apply sample limits\n        if self.config.max_train_samples:\n            train = train.select(range(min(len(train), self.config.max_train_samples)))\n        if self.config.max_val_samples:\n            validation = validation.select(range(min(len(validation), self.config.max_val_samples)))\n        if self.config.max_test_samples:\n            test = test.select(range(min(len(test), self.config.max_test_samples)))\n        \n        self._datasets = DatasetDict({\n            'train': train,\n            'validation': validation,\n            'test': test\n        })\n        \n        logger.info(f\"Dataset sizes - Train: {len(train)}, Val: {len(validation)}, Test: {len(test)}\")\n        \n        return self._datasets\n    \n    def get_datasets(self) -> DatasetDict:\n        \"\"\"Get prepared datasets.\"\"\"\n        return self.prepare_datasets()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Trainers\n",
    "\n",
    "Abstract base class and implementations for different STT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSTTTrainer(ABC):\n",
    "    \"\"\"Abstract base class for STT model trainers.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: ModelConfig, dataset_config: DatasetConfig):\n",
    "        self.model_config = model_config\n",
    "        self.dataset_config = dataset_config\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.wer_metric = evaluate.load(\"wer\")\n",
    "        self.cer_metric = evaluate.load(\"cer\")\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the model and processor.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def preprocess_dataset(self, dataset: DatasetDict) -> DatasetDict:\n",
    "        \"\"\"Preprocess dataset for this specific model.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def train(self, dataset: DatasetDict) -> Dict[str, Any]:\n",
    "        \"\"\"Train the model and return results.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def evaluate(self, dataset: Dataset) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate the model on a dataset.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def compute_metrics(self, predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Compute WER and CER metrics.\"\"\"\n",
    "        # Normalize texts\n",
    "        predictions = [pred.lower().strip() for pred in predictions]\n",
    "        references = [ref.lower().strip() for ref in references]\n",
    "        \n",
    "        metrics = {}\n",
    "        metrics['wer'] = self.wer_metric.compute(predictions=predictions, references=references)\n",
    "        metrics['cer'] = self.cer_metric.compute(predictions=predictions, references=references)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save the fine-tuned model.\"\"\"\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        self.model.save_pretrained(path)\n",
    "        if self.processor:\n",
    "            self.processor.save_pretrained(path)\n",
    "        logger.info(f\"Model saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperTrainer(BaseSTTTrainer):\n",
    "    \"\"\"Trainer for OpenAI Whisper models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_config: ModelConfig, dataset_config: DatasetConfig):\n",
    "        super().__init__(model_config, dataset_config)\n",
    "        self.data_collator = None\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load Whisper model and processor.\"\"\"\n",
    "        logger.info(f\"Loading Whisper model: {self.model_config.model_name}\")\n",
    "        \n",
    "        self.processor = WhisperProcessor.from_pretrained(self.model_config.model_name)\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(\n",
    "            self.model_config.model_name\n",
    "        )\n",
    "        \n",
    "        # Configure model\n",
    "        self.model.config.forced_decoder_ids = None\n",
    "        self.model.config.suppress_tokens = []\n",
    "        self.model.config.use_cache = False\n",
    "        \n",
    "        # Freeze encoder if specified\n",
    "        if self.model_config.freeze_encoder:\n",
    "            logger.info(\"Freezing encoder\")\n",
    "            for param in self.model.model.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif self.model_config.freeze_encoder_layers > 0:\n",
    "            logger.info(f\"Freezing first {self.model_config.freeze_encoder_layers} encoder layers\")\n",
    "            for i, layer in enumerate(self.model.model.encoder.layers):\n",
    "                if i < self.model_config.freeze_encoder_layers:\n",
    "                    for param in layer.parameters():\n",
    "                        param.requires_grad = False\n",
    "        \n",
    "        # Create data collator\n",
    "        self.data_collator = self._create_data_collator()\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def _create_data_collator(self):\n",
    "        \"\"\"Create data collator for Whisper.\"\"\"\n",
    "        from dataclasses import dataclass\n",
    "        from typing import Any, Dict, List, Union\n",
    "        \n",
    "        @dataclass\n",
    "        class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "            processor: Any\n",
    "            decoder_start_token_id: int\n",
    "\n",
    "            def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "                # Split inputs and labels\n",
    "                input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "                label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "                batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "                labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "                # Replace padding with -100\n",
    "                labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "                    labels_batch.attention_mask.ne(1), -100\n",
    "                )\n",
    "\n",
    "                # Remove BOS token if present\n",
    "                if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "                    labels = labels[:, 1:]\n",
    "\n",
    "                batch[\"labels\"] = labels\n",
    "                return batch\n",
    "        \n",
    "        return DataCollatorSpeechSeq2SeqWithPadding(\n",
    "            processor=self.processor,\n",
    "            decoder_start_token_id=self.model.config.decoder_start_token_id,\n",
    "        )\n",
    "    \n",
    "    def preprocess_dataset(self, dataset: DatasetDict) -> DatasetDict:\n",
    "        \"\"\"Preprocess dataset for Whisper.\"\"\"\n",
    "        logger.info(\"Preprocessing dataset for Whisper\")\n",
    "        \n",
    "        def prepare_dataset(batch):\n",
    "            audio = batch[self.dataset_config.audio_column]\n",
    "            \n",
    "            # Compute input features\n",
    "            batch[\"input_features\"] = self.processor.feature_extractor(\n",
    "                audio[\"array\"],\n",
    "                sampling_rate=audio[\"sampling_rate\"]\n",
    "            ).input_features[0]\n",
    "            \n",
    "            # Encode target text\n",
    "            batch[\"labels\"] = self.processor.tokenizer(\n",
    "                batch[self.dataset_config.text_column]\n",
    "            ).input_ids\n",
    "            \n",
    "            return batch\n",
    "        \n",
    "        processed = dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=dataset['train'].column_names,\n",
    "            num_proc=1,  # Audio processing doesn't parallelize well\n",
    "        )\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def train(self, dataset: DatasetDict) -> Dict[str, Any]:\n",
    "        \"\"\"Train Whisper model.\"\"\"\n",
    "        logger.info(\"Starting Whisper training\")\n",
    "        \n",
    "        # Preprocess dataset\n",
    "        processed_dataset = self.preprocess_dataset(dataset)\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=self.model_config.output_dir,\n",
    "            per_device_train_batch_size=self.model_config.batch_size,\n",
    "            per_device_eval_batch_size=self.model_config.batch_size,\n",
    "            gradient_accumulation_steps=self.model_config.gradient_accumulation_steps,\n",
    "            learning_rate=self.model_config.learning_rate,\n",
    "            warmup_steps=self.model_config.warmup_steps,\n",
    "            num_train_epochs=self.model_config.num_epochs,\n",
    "            weight_decay=self.model_config.weight_decay,\n",
    "            fp16=self.model_config.fp16 and torch.cuda.is_available(),\n",
    "            bf16=self.model_config.bf16 and torch.cuda.is_available(),\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=self.model_config.eval_steps,\n",
    "            save_steps=self.model_config.save_steps,\n",
    "            logging_steps=self.model_config.logging_steps,\n",
    "            save_total_limit=3,\n",
    "            predict_with_generate=True,\n",
    "            generation_max_length=225,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"wer\",\n",
    "            greater_is_better=False,\n",
    "            push_to_hub=False,\n",
    "            report_to=[\"tensorboard\"],\n",
    "        )\n",
    "        \n",
    "        # Compute metrics function\n",
    "        def compute_metrics(pred):\n",
    "            pred_ids = pred.predictions\n",
    "            label_ids = pred.label_ids\n",
    "            \n",
    "            # Replace -100 with pad token id\n",
    "            label_ids[label_ids == -100] = self.processor.tokenizer.pad_token_id\n",
    "            \n",
    "            # Decode predictions and labels\n",
    "            pred_str = self.processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "            label_str = self.processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "            \n",
    "            return self.compute_metrics(pred_str, label_str)\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=processed_dataset['train'],\n",
    "            eval_dataset=processed_dataset['validation'],\n",
    "            data_collator=self.data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            processing_class=self.processor.feature_extractor,\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        trainer.save_model()\n",
    "        self.processor.save_pretrained(self.model_config.output_dir)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_results = trainer.evaluate(processed_dataset['test'])\n",
    "        \n",
    "        return {\n",
    "            'train_results': train_result,\n",
    "            'test_results': test_results,\n",
    "            'model_path': self.model_config.output_dir\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, dataset: Dataset) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate model on dataset.\"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for sample in dataset:\n",
    "            audio = sample[self.dataset_config.audio_column]\n",
    "            input_features = self.processor.feature_extractor(\n",
    "                audio[\"array\"],\n",
    "                sampling_rate=audio[\"sampling_rate\"],\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predicted_ids = self.model.generate(input_features)\n",
    "            \n",
    "            transcription = self.processor.batch_decode(\n",
    "                predicted_ids, skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            predictions.append(transcription)\n",
    "            references.append(sample[self.dataset_config.text_column])\n",
    "        \n",
    "        return self.compute_metrics(predictions, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ParakeetTrainer(BaseSTTTrainer):\n    \"\"\"Trainer for NVIDIA Parakeet models using NeMo.\"\"\"\n    \n    def __init__(self, model_config: ModelConfig, dataset_config: DatasetConfig):\n        super().__init__(model_config, dataset_config)\n        self.nemo_trainer = None\n    \n    def load_model(self):\n        \"\"\"Load Parakeet model.\"\"\"\n        logger.info(f\"Loading Parakeet model: {self.model_config.model_name}\")\n        \n        try:\n            import nemo.collections.asr as nemo_asr\n            from omegaconf import OmegaConf, open_dict\n        except ImportError:\n            raise ImportError(\"NeMo toolkit not installed. Run: pip install nemo_toolkit[asr]\")\n        \n        # Load pre-trained model\n        if self.model_config.model_name.endswith('.nemo'):\n            self.model = nemo_asr.models.ASRModel.restore_from(self.model_config.model_name)\n        else:\n            self.model = nemo_asr.models.ASRModel.from_pretrained(self.model_config.model_name)\n        \n        # Freeze encoder if specified\n        if self.model_config.freeze_encoder:\n            logger.info(\"Freezing encoder\")\n            self.model.encoder.freeze()\n        \n        return self.model\n    \n    def _create_manifest(self, dataset: Dataset, manifest_path: str):\n        \"\"\"Create NeMo manifest file from dataset.\"\"\"\n        import soundfile as sf\n        \n        manifest_dir = Path(manifest_path).parent\n        audio_dir = manifest_dir / \"audio\"\n        audio_dir.mkdir(parents=True, exist_ok=True)\n        \n        entries = []\n        for idx, sample in enumerate(dataset):\n            audio = sample[self.dataset_config.audio_column]\n            text = sample[self.dataset_config.text_column]\n            \n            # Save audio file\n            audio_path = audio_dir / f\"audio_{idx}.wav\"\n            sf.write(str(audio_path), audio['array'], audio['sampling_rate'])\n            \n            # Create manifest entry\n            duration = len(audio['array']) / audio['sampling_rate']\n            entries.append({\n                \"audio_filepath\": str(audio_path),\n                \"text\": text,\n                \"duration\": duration\n            })\n        \n        # Write manifest\n        with open(manifest_path, 'w') as f:\n            for entry in entries:\n                f.write(json.dumps(entry) + '\\n')\n        \n        logger.info(f\"Created manifest with {len(entries)} entries at {manifest_path}\")\n        return manifest_path\n    \n    def preprocess_dataset(self, dataset: DatasetDict) -> Dict[str, str]:\n        \"\"\"Preprocess dataset by creating NeMo manifest files.\"\"\"\n        logger.info(\"Creating NeMo manifest files\")\n        \n        manifest_dir = Path(self.model_config.output_dir) / \"manifests\"\n        manifest_dir.mkdir(parents=True, exist_ok=True)\n        \n        manifests = {}\n        for split in ['train', 'validation', 'test']:\n            manifest_path = manifest_dir / f\"{split}_manifest.json\"\n            self._create_manifest(dataset[split], str(manifest_path))\n            manifests[split] = str(manifest_path)\n        \n        return manifests\n    \n    def train(self, dataset: DatasetDict) -> Dict[str, Any]:\n        \"\"\"Train Parakeet model using manual training loop (PyTorch Lightning compatibility workaround).\"\"\"\n        logger.info(\"Starting Parakeet training\")\n        \n        try:\n            from omegaconf import OmegaConf, open_dict\n        except ImportError as e:\n            raise ImportError(f\"Required packages not installed: {e}\")\n        \n        # Create manifest files\n        manifests = self.preprocess_dataset(dataset)\n        \n        # Create output directory\n        output_dir = Path(self.model_config.output_dir)\n        output_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Update model config for fine-tuning using OmegaConf\n        with open_dict(self.model.cfg):\n            # Update training data\n            self.model.cfg.train_ds.manifest_filepath = manifests['train']\n            self.model.cfg.train_ds.batch_size = self.model_config.batch_size\n            \n            # Update validation data  \n            self.model.cfg.validation_ds.manifest_filepath = manifests['validation']\n            self.model.cfg.validation_ds.batch_size = self.model_config.batch_size\n            \n            # Update optimizer\n            self.model.cfg.optim.lr = self.model_config.learning_rate\n        \n        # Setup data loaders\n        self.model.setup_training_data(self.model.cfg.train_ds)\n        self.model.setup_validation_data(self.model.cfg.validation_ds)\n        \n        # Use manual training loop (bypasses PyTorch Lightning compatibility issues)\n        self._train_manual()\n        \n        # Save model\n        model_path = output_dir / \"parakeet_finetuned.nemo\"\n        self.model.save_to(str(model_path))\n        logger.info(f\"Model saved to {model_path}\")\n        \n        # Evaluate on test set\n        test_results = self.evaluate(dataset['test'])\n        \n        return {\n            'model_path': str(model_path),\n            'test_results': test_results\n        }\n    \n    def _train_manual(self):\n        \"\"\"Manual training loop that bypasses PyTorch Lightning trainer.fit() issues.\"\"\"\n        logger.info(\"Using manual training loop for NeMo model\")\n        \n        # Move model to device\n        self.model = self.model.to(device)\n        self.model.train()\n        \n        # Setup optimizer\n        optimizer = torch.optim.AdamW(\n            self.model.parameters(), \n            lr=self.model_config.learning_rate,\n            weight_decay=self.model_config.weight_decay,\n            betas=(0.9, 0.98)\n        )\n        \n        # Get the training dataloader\n        train_dl = self.model._train_dl\n        if train_dl is None:\n            raise ValueError(\"Training dataloader not set up. Call setup_training_data first.\")\n        \n        # Setup mixed precision if enabled\n        scaler = torch.cuda.amp.GradScaler() if self.model_config.fp16 and torch.cuda.is_available() else None\n        \n        total_steps = 0\n        best_val_loss = float('inf')\n        \n        for epoch in range(self.model_config.num_epochs):\n            logger.info(f\"Epoch {epoch + 1}/{self.model_config.num_epochs}\")\n            \n            epoch_loss = 0.0\n            num_batches = 0\n            self.model.train()\n            \n            for batch_idx, batch in enumerate(train_dl):\n                # Move batch to device - handle different batch formats\n                if isinstance(batch, (list, tuple)):\n                    batch = [b.to(device) if isinstance(b, torch.Tensor) else b for b in batch]\n                elif isinstance(batch, dict):\n                    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                \n                optimizer.zero_grad()\n                \n                try:\n                    if scaler is not None:\n                        # Mixed precision training\n                        with torch.cuda.amp.autocast():\n                            loss, _ = self.model.training_step(batch, batch_idx)\n                        scaler.scale(loss).backward()\n                        scaler.step(optimizer)\n                        scaler.update()\n                    else:\n                        # Standard training\n                        loss, _ = self.model.training_step(batch, batch_idx)\n                        if isinstance(loss, dict):\n                            loss = loss.get('loss', loss.get('val_loss', list(loss.values())[0]))\n                        loss.backward()\n                        optimizer.step()\n                    \n                    loss_value = loss.item() if hasattr(loss, 'item') else float(loss)\n                    epoch_loss += loss_value\n                    num_batches += 1\n                    total_steps += 1\n                    \n                    if batch_idx % self.model_config.logging_steps == 0:\n                        logger.info(f\"  Step {total_steps}, Batch {batch_idx}, Loss: {loss_value:.4f}\")\n                        \n                except Exception as e:\n                    logger.warning(f\"Error in batch {batch_idx}: {e}\")\n                    continue\n            \n            # Epoch summary\n            avg_loss = epoch_loss / max(num_batches, 1)\n            logger.info(f\"  Epoch {epoch + 1} - Avg Train Loss: {avg_loss:.4f}\")\n            \n            # Validation\n            val_loss = self._validate()\n            logger.info(f\"  Epoch {epoch + 1} - Val Loss: {val_loss:.4f}\")\n            \n            # Save best model\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                checkpoint_path = Path(self.model_config.output_dir) / \"best_checkpoint.nemo\"\n                self.model.save_to(str(checkpoint_path))\n                logger.info(f\"  New best model saved (val_loss: {val_loss:.4f})\")\n        \n        logger.info(f\"Training complete! Best validation loss: {best_val_loss:.4f}\")\n    \n    def _validate(self) -> float:\n        \"\"\"Run validation and return average loss.\"\"\"\n        self.model.eval()\n        val_dl = self.model._validation_dl\n        \n        if val_dl is None:\n            return float('inf')\n        \n        total_loss = 0.0\n        num_batches = 0\n        \n        with torch.no_grad():\n            for batch_idx, batch in enumerate(val_dl):\n                # Move batch to device\n                if isinstance(batch, (list, tuple)):\n                    batch = [b.to(device) if isinstance(b, torch.Tensor) else b for b in batch]\n                elif isinstance(batch, dict):\n                    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n                \n                try:\n                    loss = self.model.validation_step(batch, batch_idx)\n                    if isinstance(loss, dict):\n                        loss = loss.get('val_loss', loss.get('loss', list(loss.values())[0]))\n                    \n                    loss_value = loss.item() if hasattr(loss, 'item') else float(loss)\n                    total_loss += loss_value\n                    num_batches += 1\n                except Exception as e:\n                    logger.warning(f\"Validation error in batch {batch_idx}: {e}\")\n                    continue\n        \n        self.model.train()\n        return total_loss / max(num_batches, 1)\n    \n    def evaluate(self, dataset: Dataset) -> Dict[str, float]:\n        \"\"\"Evaluate model on dataset.\"\"\"\n        logger.info(\"Evaluating Parakeet model...\")\n        \n        self.model.eval()\n        self.model = self.model.to(device)\n        \n        predictions = []\n        references = []\n        \n        for idx, sample in enumerate(dataset):\n            audio = sample[self.dataset_config.audio_column]\n            \n            try:\n                # Transcribe using the model\n                transcription = self.model.transcribe(\n                    [audio['array']],\n                    batch_size=1\n                )\n                \n                # Handle different return types\n                if isinstance(transcription, list):\n                    transcription = transcription[0]\n                if isinstance(transcription, tuple):\n                    transcription = transcription[0]\n                    \n                predictions.append(str(transcription))\n                references.append(sample[self.dataset_config.text_column])\n                \n            except Exception as e:\n                logger.warning(f\"Failed to transcribe sample {idx}: {e}\")\n                continue\n        \n        if not predictions:\n            logger.warning(\"No successful predictions made\")\n            return {'wer': 1.0, 'cer': 1.0}\n        \n        return self.compute_metrics(predictions, references)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model factory for creating trainers\n",
    "class ModelFactory:\n",
    "    \"\"\"Factory for creating model trainers.\"\"\"\n",
    "    \n",
    "    _trainers = {\n",
    "        'whisper': WhisperTrainer,\n",
    "        'parakeet': ParakeetTrainer,\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def register(cls, model_type: str, trainer_class: type):\n",
    "        \"\"\"Register a new model trainer.\"\"\"\n",
    "        cls._trainers[model_type] = trainer_class\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, model_config: ModelConfig, dataset_config: DatasetConfig) -> BaseSTTTrainer:\n",
    "        \"\"\"Create a trainer for the specified model type.\"\"\"\n",
    "        model_type = model_config.model_type.lower()\n",
    "        \n",
    "        if model_type not in cls._trainers:\n",
    "            raise ValueError(\n",
    "                f\"Unknown model type: {model_type}. \"\n",
    "                f\"Available types: {list(cls._trainers.keys())}\"\n",
    "            )\n",
    "        \n",
    "        return cls._trainers[model_type](model_config, dataset_config)\n",
    "    \n",
    "    @classmethod\n",
    "    def available_models(cls) -> List[str]:\n",
    "        \"\"\"List available model types.\"\"\"\n",
    "        return list(cls._trainers.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline\n",
    "\n",
    "Orchestrates training with optional parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    \"\"\"Main training pipeline with parallel training support.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.dataset_handler = DatasetHandler(config.dataset)\n",
    "        self.results: Dict[str, Any] = {}\n",
    "        \n",
    "        # Set random seeds\n",
    "        self._set_seed(config.seed)\n",
    "    \n",
    "    def _set_seed(self, seed: int):\n",
    "        \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "        import random\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    def train_single_model(self, model_config: ModelConfig) -> Dict[str, Any]:\n",
    "        \"\"\"Train a single model.\"\"\"\n",
    "        logger.info(f\"Training model: {model_config.model_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Create trainer\n",
    "            trainer = ModelFactory.create(model_config, self.config.dataset)\n",
    "            \n",
    "            # Load model\n",
    "            trainer.load_model()\n",
    "            \n",
    "            # Get datasets\n",
    "            datasets = self.dataset_handler.get_datasets()\n",
    "            \n",
    "            # Train\n",
    "            results = trainer.train(datasets)\n",
    "            results['status'] = 'success'\n",
    "            results['model_name'] = model_config.model_name\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training {model_config.model_name}: {str(e)}\")\n",
    "            return {\n",
    "                'status': 'failed',\n",
    "                'model_name': model_config.model_name,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def train_parallel(self) -> Dict[str, Any]:\n",
    "        \"\"\"Train multiple models in parallel.\"\"\"\n",
    "        logger.info(f\"Starting parallel training of {len(self.config.models)} models\")\n",
    "        \n",
    "        # Note: For GPU training, we use ThreadPoolExecutor since\n",
    "        # ProcessPoolExecutor can have issues with CUDA contexts\n",
    "        results = {}\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.config.max_parallel_models) as executor:\n",
    "            future_to_model = {\n",
    "                executor.submit(self.train_single_model, model_config): model_config\n",
    "                for model_config in self.config.models\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_model):\n",
    "                model_config = future_to_model[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results[model_config.model_name] = result\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Training failed for {model_config.model_name}: {e}\")\n",
    "                    results[model_config.model_name] = {\n",
    "                        'status': 'failed',\n",
    "                        'error': str(e)\n",
    "                    }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def train_sequential(self) -> Dict[str, Any]:\n",
    "        \"\"\"Train models sequentially.\"\"\"\n",
    "        logger.info(f\"Starting sequential training of {len(self.config.models)} models\")\n",
    "        \n",
    "        results = {}\n",
    "        for model_config in self.config.models:\n",
    "            result = self.train_single_model(model_config)\n",
    "            results[model_config.model_name] = result\n",
    "            \n",
    "            # Clear GPU memory between models\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run the training pipeline.\"\"\"\n",
    "        logger.info(\"Starting training pipeline\")\n",
    "        \n",
    "        # Load datasets first (shared across all models)\n",
    "        datasets = self.dataset_handler.get_datasets()\n",
    "        logger.info(f\"Datasets loaded: {datasets}\")\n",
    "        \n",
    "        # Train models\n",
    "        if self.config.enable_parallel and len(self.config.models) > 1:\n",
    "            self.results = self.train_parallel()\n",
    "        else:\n",
    "            self.results = self.train_sequential()\n",
    "        \n",
    "        # Summary\n",
    "        self._print_summary()\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def _print_summary(self):\n",
    "        \"\"\"Print training summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TRAINING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for model_name, result in self.results.items():\n",
    "            print(f\"\\nModel: {model_name}\")\n",
    "            print(f\"  Status: {result.get('status', 'unknown')}\")\n",
    "            \n",
    "            if result.get('status') == 'success':\n",
    "                test_results = result.get('test_results', {})\n",
    "                if 'wer' in test_results or 'eval_wer' in test_results:\n",
    "                    wer = test_results.get('wer', test_results.get('eval_wer', 'N/A'))\n",
    "                    print(f\"  Test WER: {wer:.4f}\" if isinstance(wer, float) else f\"  Test WER: {wer}\")\n",
    "                if 'cer' in test_results or 'eval_cer' in test_results:\n",
    "                    cer = test_results.get('cer', test_results.get('eval_cer', 'N/A'))\n",
    "                    print(f\"  Test CER: {cer:.4f}\" if isinstance(cer, float) else f\"  Test CER: {cer}\")\n",
    "                print(f\"  Model saved: {result.get('model_path', 'N/A')}\")\n",
    "            else:\n",
    "                print(f\"  Error: {result.get('error', 'Unknown error')}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify configuration\n",
    "print(\"Configuration Summary:\")\n",
    "print(f\"  Models to train: {len(CONFIG.models)}\")\n",
    "for m in CONFIG.models:\n",
    "    print(f\"    - {m.model_type}: {m.model_name}\")\n",
    "print(f\"  Training datasets: {CONFIG.dataset.train_datasets}\")\n",
    "print(f\"  Parallel training: {CONFIG.enable_parallel}\")\n",
    "print(f\"  Output directory: {CONFIG.output_base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run the training pipeline\n",
    "pipeline = TrainingPipeline(CONFIG)\n",
    "results = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results\n",
    "import pandas as pd\n",
    "\n",
    "def create_results_table(results: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Create a results summary table.\"\"\"\n",
    "    rows = []\n",
    "    for model_name, result in results.items():\n",
    "        row = {\n",
    "            'Model': model_name,\n",
    "            'Status': result.get('status', 'unknown'),\n",
    "        }\n",
    "        \n",
    "        if result.get('status') == 'success':\n",
    "            test_results = result.get('test_results', {})\n",
    "            row['WER'] = test_results.get('wer', test_results.get('eval_wer', None))\n",
    "            row['CER'] = test_results.get('cer', test_results.get('eval_cer', None))\n",
    "            row['Model Path'] = result.get('model_path', '')\n",
    "        else:\n",
    "            row['Error'] = result.get('error', '')\n",
    "        \n",
    "        rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "results_df = create_results_table(results)\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results_path = Path(CONFIG.output_base_dir) / \"training_results.json\"\n",
    "\n",
    "# Convert results to JSON-serializable format\n",
    "def make_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: make_serializable(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [make_serializable(v) for v in obj]\n",
    "    elif isinstance(obj, (np.integer, np.floating)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        return make_serializable(obj.__dict__)\n",
    "    return obj\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(make_serializable(results), f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(model_path: str, model_type: str = \"whisper\"):\n",
    "    \"\"\"Load a fine-tuned model for inference.\"\"\"\n",
    "    if model_type == \"whisper\":\n",
    "        processor = WhisperProcessor.from_pretrained(model_path)\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "        model.to(device)\n",
    "        return processor, model\n",
    "    elif model_type == \"parakeet\":\n",
    "        import nemo.collections.asr as nemo_asr\n",
    "        model = nemo_asr.models.ASRModel.restore_from(model_path)\n",
    "        return None, model\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_path: str, processor, model, model_type: str = \"whisper\") -> str:\n",
    "    \"\"\"Transcribe audio using a fine-tuned model.\"\"\"\n",
    "    if model_type == \"whisper\":\n",
    "        # Load audio\n",
    "        audio, sr = torchaudio.load(audio_path)\n",
    "        if sr != 16000:\n",
    "            audio = torchaudio.functional.resample(audio, sr, 16000)\n",
    "        \n",
    "        # Prepare input\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio.squeeze().numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(input_features)\n",
    "        \n",
    "        # Decode\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "        return transcription\n",
    "    \n",
    "    elif model_type == \"parakeet\":\n",
    "        transcription = model.transcribe([audio_path])[0]\n",
    "        return transcription\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load fine-tuned model and transcribe\n",
    "# Uncomment and modify the path to use\n",
    "\n",
    "# model_path = \"./outputs/whisper-small-finetuned\"\n",
    "# processor, model = load_finetuned_model(model_path, model_type=\"whisper\")\n",
    "\n",
    "# # Transcribe a test audio file\n",
    "# audio_path = \"path/to/your/audio.wav\"\n",
    "# transcription = transcribe_audio(audio_path, processor, model, model_type=\"whisper\")\n",
    "# print(f\"Transcription: {transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Adding Custom Models\n",
    "\n",
    "To add support for a new STT model, create a class that inherits from `BaseSTTTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adding a custom model trainer\n",
    "\n",
    "class CustomModelTrainer(BaseSTTTrainer):\n",
    "    \"\"\"Template for adding a custom STT model trainer.\"\"\"\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Load your custom model.\"\"\"\n",
    "        # self.model = YourModel.from_pretrained(self.model_config.model_name)\n",
    "        # self.processor = YourProcessor.from_pretrained(self.model_config.model_name)\n",
    "        raise NotImplementedError(\"Implement model loading for your custom model\")\n",
    "    \n",
    "    def preprocess_dataset(self, dataset: DatasetDict) -> DatasetDict:\n",
    "        \"\"\"Preprocess dataset for your model.\"\"\"\n",
    "        # Implement preprocessing specific to your model\n",
    "        raise NotImplementedError(\"Implement preprocessing for your custom model\")\n",
    "    \n",
    "    def train(self, dataset: DatasetDict) -> Dict[str, Any]:\n",
    "        \"\"\"Train your model.\"\"\"\n",
    "        # Implement training logic\n",
    "        raise NotImplementedError(\"Implement training for your custom model\")\n",
    "    \n",
    "    def evaluate(self, dataset: Dataset) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate your model.\"\"\"\n",
    "        # Implement evaluation logic\n",
    "        raise NotImplementedError(\"Implement evaluation for your custom model\")\n",
    "\n",
    "\n",
    "# Register the custom model\n",
    "# ModelFactory.register('custom', CustomModelTrainer)\n",
    "\n",
    "# Then use it in configuration:\n",
    "# ModelConfig(\n",
    "#     model_type=\"custom\",\n",
    "#     model_name=\"your/model-name\",\n",
    "#     ...\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(results: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Compare performance of trained models.\"\"\"\n",
    "    comparison = []\n",
    "    \n",
    "    for model_name, result in results.items():\n",
    "        if result.get('status') == 'success':\n",
    "            test_results = result.get('test_results', {})\n",
    "            comparison.append({\n",
    "                'Model': model_name,\n",
    "                'WER': test_results.get('wer', test_results.get('eval_wer')),\n",
    "                'CER': test_results.get('cer', test_results.get('eval_cer')),\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(comparison)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values('WER')\n",
    "    return df\n",
    "\n",
    "\n",
    "def export_config(config: TrainingConfig, path: str):\n",
    "    \"\"\"Export configuration to JSON file.\"\"\"\n",
    "    config_dict = asdict(config)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2, default=str)\n",
    "    print(f\"Configuration exported to: {path}\")\n",
    "\n",
    "\n",
    "def load_config(path: str) -> TrainingConfig:\n",
    "    \"\"\"Load configuration from JSON file.\"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        config_dict = json.load(f)\n",
    "    \n",
    "    # Reconstruct dataclasses\n",
    "    models = [ModelConfig(**m) for m in config_dict.pop('models', [])]\n",
    "    dataset = DatasetConfig(**config_dict.pop('dataset', {}))\n",
    "    \n",
    "    return TrainingConfig(models=models, dataset=dataset, **config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare trained models\n",
    "if results:\n",
    "    comparison_df = compare_models(results)\n",
    "    if not comparison_df.empty:\n",
    "        print(\"Model Comparison (sorted by WER):\")\n",
    "        display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export configuration for reproducibility\n",
    "config_export_path = Path(CONFIG.output_base_dir) / \"training_config.json\"\n",
    "export_config(CONFIG, str(config_export_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Quick Reference\n\n### Adding a New Dataset\n```python\nDATASET_CONFIG = DatasetConfig(\n    train_datasets=[\n        \"mozilla-foundation/common_voice_11_0\",  # HuggingFace dataset\n        \"/path/to/local/data\",                   # Local directory\n        \"/path/to/manifest.json\",                # JSON manifest\n    ],\n    audio_column=\"audio\",\n    text_column=\"sentence\",\n    dataset_config_name=\"en\",  # Language code for Common Voice, None for others\n    dataset_split=\"train\",\n)\n```\n\n### Adding a New Model\n```python\nMODELS_TO_TRAIN.append(\n    ModelConfig(\n        model_type=\"whisper\",  # or \"parakeet\", or custom\n        model_name=\"openai/whisper-large-v3\",\n        learning_rate=1e-5,\n        batch_size=4,\n        num_epochs=5,\n    )\n)\n```\n\n### Enabling Parallel Training\n```python\nCONFIG = TrainingConfig(\n    ...\n    enable_parallel=True,\n    max_parallel_models=2,\n)\n```\n\n### Using Separate Validation/Test Sets\n```python\nDATASET_CONFIG = DatasetConfig(\n    train_datasets=[\"dataset/train\"],\n    validation_dataset=\"dataset/validation\",\n    test_dataset=\"dataset/test\",\n    dataset_config_name=None,  # Set if needed\n)\n```\n\n### Common Dataset Examples\n```python\n# Common Voice (requires language config)\nDATASET_CONFIG = DatasetConfig(\n    train_datasets=[\"mozilla-foundation/common_voice_11_0\"],\n    dataset_config_name=\"en\",  # \"es\", \"fr\", \"de\", etc.\n    dataset_split=\"train\",\n)\n\n# LibriSpeech (no config needed)\nDATASET_CONFIG = DatasetConfig(\n    train_datasets=[\"librispeech_asr\"],\n    dataset_config_name=None,\n    dataset_split=\"train.clean.100\",\n    audio_column=\"audio\",\n    text_column=\"text\",\n)\n\n# Local audiofolder\nDATASET_CONFIG = DatasetConfig(\n    train_datasets=[\"/path/to/audio/folder\"],\n    dataset_config_name=None,\n    audio_column=\"audio\",\n    text_column=\"transcription\",\n)\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}