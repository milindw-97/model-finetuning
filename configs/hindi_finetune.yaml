# Hindi Fine-Tuning Configuration for Parakeet RNNT 1.1B
#
# Optimized for small dataset (<10 hours) training on Azure T4 GPU (16GB)
#
# Usage:
#   python scripts/finetune_hindi.py --config configs/hindi_finetune.yaml

# Pre-trained model
model:
  pretrained_name: "nvidia/parakeet-rnnt-1.1b"
  # Alternative: use local .nemo file
  # pretrained_path: "/path/to/model.nemo"

# Training data configuration
train_ds:
  manifest_filepath: "data/train_manifest.json"
  batch_size: 4  # Reduced for T4 16GB memory
  num_workers: 4
  sample_rate: 16000
  max_duration: 20.0
  min_duration: 0.3
  shuffle: true
  pin_memory: true

# Validation data configuration
validation_ds:
  manifest_filepath: "data/val_manifest.json"
  batch_size: 4
  num_workers: 4
  sample_rate: 16000
  shuffle: false
  pin_memory: true

# Test data configuration (optional)
test_ds:
  manifest_filepath: "data/test_manifest.json"
  batch_size: 4
  num_workers: 4
  sample_rate: 16000

# Optimizer configuration
# Lower learning rate for small dataset to prevent overfitting
optim:
  name: adamw
  lr: 5.0e-5  # Lower LR for small dataset
  betas: [0.9, 0.98]
  weight_decay: 0.001

  # Learning rate scheduler
  sched:
    name: CosineAnnealing
    warmup_steps: 100
    min_lr: 1.0e-6

# Spec augmentation (data augmentation for audio)
# Helps prevent overfitting on small datasets
spec_augment:
  freq_masks: 2
  time_masks: 10
  freq_width: 27
  time_width: 0.05

# Encoder freezing
# CRITICAL for small datasets - prevents catastrophic forgetting
encoder:
  freeze: true
  # Alternative: unfreeze after N epochs
  # unfreeze_after_epochs: 10

# Trainer configuration
trainer:
  devices: 1  # Number of GPUs
  accelerator: "gpu"
  max_epochs: 100
  precision: "bf16-mixed"

  # Gradient settings
  accumulate_grad_batches: 4  # Effective batch size = 4 * 4 = 16
  gradient_clip_val: 1.0

  # Logging
  log_every_n_steps: 10

  # Checkpointing
  enable_checkpointing: true

# Experiment manager (NeMo-style)
exp_manager:
  exp_dir: "outputs/parakeet-hindi-finetuned"
  name: "hindi_finetune"
  create_tensorboard_logger: true
  create_wandb_logger: false

  # Checkpoint settings
  checkpoint_callback_params:
    monitor: "val_wer"
    mode: "min"
    save_top_k: 3
    save_last: true

  # Early stopping
  early_stopping_callback_params:
    monitor: "val_wer"
    patience: 15
    min_delta: 0.001
    mode: "min"

# Tokenizer configuration
# Use existing multilingual tokenizer first
# If Hindi support is poor, train a new tokenizer
tokenizer:
  # Set to true to update tokenizer for Hindi
  update_tokenizer: false

  # If updating tokenizer:
  # dir: "tokenizers/hindi"
  # type: "bpe"
  # vocab_size: 256

# Output configuration
output:
  dir: "outputs/parakeet-hindi-finetuned"
  save_final_model: true
  export_onnx: false

# Reproducibility
seed: 42
